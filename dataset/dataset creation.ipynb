{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c666b8-22a0-4f5d-b699-0976ec0b9272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textstat\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Ensure required NLTK resources are available\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d727f8e-b93f-4e79-bd1c-221467a62c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c34c09b0-8513-4dce-a8a6-151dcbe015b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Utility Functions -------------------- #\n",
    "\n",
    "def compute_readability_scores(text):\n",
    "    \"\"\"Compute readability scores for a given text.\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str) or text.strip() == \"\":\n",
    "        return [None] * 6  # Return None values if text is missing\n",
    "    return [\n",
    "        textstat.flesch_reading_ease(text),\n",
    "        textstat.flesch_kincaid_grade(text),\n",
    "        textstat.gunning_fog(text),\n",
    "        textstat.smog_index(text),\n",
    "        textstat.dale_chall_readability_score(text),\n",
    "        textstat.automated_readability_index(text)\n",
    "    ]\n",
    "\n",
    "\n",
    "def extract_and_clean_zip(zip_folder_path, extract_folder_path):\n",
    "    \"\"\"Extracts ZIP files containing CSVs, removes the first row, and returns a list of DataFrames.\"\"\"\n",
    "    os.makedirs(extract_folder_path, exist_ok=True)\n",
    "    dataframes = []\n",
    "    \n",
    "    for zip_file in os.listdir(zip_folder_path):\n",
    "        if zip_file.endswith(\".zip\"):\n",
    "            zip_path = os.path.join(zip_folder_path, zip_file)\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "                for file in zf.namelist():\n",
    "                    if file.endswith(\".csv\"):\n",
    "                        with zf.open(file) as extracted_file:\n",
    "                            df = pd.read_csv(extracted_file, skiprows=1)  # Remove the first row\n",
    "                        dataframes.append(df)\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def process_domain(zip_folder, extract_folder, output_name, domain, base_dir, columns_to_keep):\n",
    "    \"\"\"Processes a domain by extracting data, cleaning it, computing readability scores, and saving it.\"\"\"\n",
    "    zip_folder_path = os.path.join(base_dir, zip_folder)\n",
    "    extract_folder_path = os.path.join(base_dir, extract_folder)\n",
    "    cleaned_csv_path = os.path.join(extract_folder_path, f\"{output_name}.csv\")\n",
    "    \n",
    "    dataframes = extract_and_clean_zip(zip_folder_path, extract_folder_path)\n",
    "    if not dataframes:\n",
    "        print(f\"No CSV files found in {zip_folder}\")\n",
    "        return None\n",
    "    \n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    if \"DOI\" in combined_df.columns:\n",
    "        combined_df = combined_df.drop_duplicates(subset=\"DOI\", keep=\"first\")\n",
    "    \n",
    "    required_columns = [\"Abstract\", \"Title\", \"FCR\"]\n",
    "    combined_df = combined_df.dropna(subset=[col for col in required_columns if col in combined_df.columns]).reset_index(drop=True)\n",
    "    \n",
    "    available_columns = [col for col in columns_to_keep if col in combined_df.columns]\n",
    "    combined_df = combined_df[available_columns]\n",
    "    \n",
    "    if \"Abstract\" in combined_df.columns:\n",
    "        readability_scores = combined_df[\"Abstract\"].apply(compute_readability_scores)\n",
    "        readability_df = pd.DataFrame(\n",
    "            readability_scores.tolist(),\n",
    "            columns=[\n",
    "                \"Flesch Reading Ease\", \"Flesch-Kincaid Grade Level\", \"Gunning Fog Index\",\n",
    "                \"SMOG Index\", \"Dale-Chall Readability Score\", \"Automated Readability Index\"\n",
    "            ]\n",
    "        )\n",
    "        combined_df = pd.concat([combined_df, readability_df], axis=1)\n",
    "    \n",
    "    combined_df[\"Domain\"] = domain\n",
    "    combined_df.to_csv(cleaned_csv_path, index=False)\n",
    "    return cleaned_csv_path\n",
    "\n",
    "\n",
    "def merge_cleaned_data(cleaned_file_paths, final_output_path):\n",
    "    \"\"\"Merges all cleaned datasets into one final dataset and removes duplicate DOIs.\"\"\"\n",
    "    dataframes = [pd.read_csv(file) for file in cleaned_file_paths]\n",
    "    final_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    if \"DOI\" in final_df.columns:\n",
    "        final_df = final_df.drop_duplicates(subset=\"DOI\", keep=\"first\")\n",
    "    \n",
    "    final_df.to_csv(final_output_path, index=False)\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def ecdf_normalization(values):\n",
    "    \"\"\"Computes ECDF normalization for a given array of values.\"\"\"\n",
    "    sorted_vals = np.sort(values)\n",
    "    ecdf_scores = np.searchsorted(sorted_vals, values, side='right') / len(sorted_vals)\n",
    "    return ecdf_scores.round(3)\n",
    "\n",
    "\n",
    "def add_ecdf_normalization(df):\n",
    "    \"\"\"Adds ECDF normalization to the dataset per domain and year.\"\"\"\n",
    "    df['ECDF_FCR'] = df.groupby(['Domain', 'PubYear'])['FCR'].transform(ecdf_normalization)\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans text by lowercasing and removing special characters.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s.,]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def filter_short_abstracts(df):\n",
    "    \"\"\"Removes abstracts that have fewer than 100 tokens.\"\"\"\n",
    "    df[\"Token Count\"] = df[\"Abstract\"].apply(lambda x: len(word_tokenize(x)) if isinstance(x, str) else 0)\n",
    "    return df[df[\"Token Count\"] >= 100].drop(columns=[\"Token Count\"])\n",
    "\n",
    "def remove_empty_titles_abstracts(df):\n",
    "    \"\"\"Removes rows where 'Title' or 'Abstract' is empty, NaN, or Title has fewer than 4 tokens.\"\"\"\n",
    "    df = df.dropna(subset=[\"Title\", \"Abstract\"])\n",
    "    df[\"Title Token Count\"] = df[\"Title\"].apply(lambda x: len(word_tokenize(x)) if isinstance(x, str) else 0)\n",
    "    return df[df[\"Title Token Count\"] >= 4].drop(columns=[\"Title Token Count\"])\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_dataset(df):\n",
    "    \"\"\"Applies text cleaning, computes readability, and filters data.\"\"\"\n",
    "    df['Title'] = df['Title'].apply(clean_text)\n",
    "    df['Abstract'] = df['Abstract'].apply(clean_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "def categorize_fcr(df):\n",
    "    \"\"\"Categorizes ECDF_FCR into three bins: Low, Medium, and High.\"\"\"\n",
    "    bins = [0, 0.33, 0.67, 1]\n",
    "    labels = [\"Low\", \"Medium\", \"High\"]\n",
    "    df[\"ECDF_FCR_Category\"] = pd.cut(df[\"ECDF_FCR\"], bins=bins, labels=labels, include_lowest=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def balance_dataset(df):\n",
    "    \"\"\"Balances dataset by undersampling each category to match the smallest group.\"\"\"\n",
    "\n",
    "    category_counts = df[\"ECDF_FCR_Category\"].value_counts()\n",
    "         \n",
    "    # Print category counts\n",
    "    print(\"Category Distribution Before Balanced:\\n\", category_counts)\n",
    "\n",
    "    \n",
    "    min_count = df[\"ECDF_FCR_Category\"].value_counts().min()\n",
    "    #df_balanced = df.groupby(\"ECDF_FCR_Category\").apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n",
    "\n",
    "    df_balanced = df.groupby(\"ECDF_FCR_Category\", observed=False, group_keys=False).apply(\n",
    "        lambda x: x.sample(min_count)\n",
    "    ).reset_index(drop=True)[df.columns]  # Ensures original column structure\n",
    "\n",
    "    \n",
    "    # Verify new distribution\n",
    "    print(\"Balanced Category Distribution:\\n\", df_balanced[\"ECDF_FCR_Category\"].value_counts())\n",
    "    return df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63846a43-7d12-4860-b855-0db2287b2e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d00fa5d2-6f3e-454a-8f9d-ba58ad1e86fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Main Execution -------------------- #\n",
    "\n",
    "base_dir = \"../Dataset Creation\"\n",
    "final_combined_csv_path = os.path.join(base_dir, \"2018-2022 Data.csv\")\n",
    "\n",
    "zip_folders = [\"zip folders/CS_zip\", \"zip folders/Eng_zip\", \"zip folders/Math_zip\", \"zip folders/Psych_zip\", \"zip folders/Physical Sci_zip\"]\n",
    "extract_folders = [\"computer science\", \"engineering\", \"math\", \"psychology\", \"physical science\"]\n",
    "combined_output = [\"computer science data\", \"engineering data\", \"math data\", \"psychology data\", \"physical science data\"]\n",
    "domains = [\"Computer Science\", \"Engineering\", \"Mathematics\", \"Psychology\", \"Physical Science\"]\n",
    "\n",
    "columns_to_keep = [\"Publication ID\", \"DOI\", \"Title\", \"Abstract\", \"FCR\", \"PubYear\"]\n",
    "cleaned_file_paths = []\n",
    "\n",
    "for zip_folder, extract_folder, output_name, domain in zip(zip_folders, extract_folders, combined_output, domains):\n",
    "    cleaned_path = process_domain(zip_folder, extract_folder, output_name, domain, base_dir, columns_to_keep)\n",
    "    if cleaned_path:\n",
    "        cleaned_file_paths.append(cleaned_path)\n",
    "\n",
    "final_df1 = merge_cleaned_data(cleaned_file_paths, final_combined_csv_path)\n",
    "final_df1.to_csv(final_combined_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d17a33e-6fde-42dc-9ba8-c7b4f90062a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d605dfe6-a79c-4000-8afe-6c7113d65a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45231"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb17778-ec01-4f69-a43f-caf74615e68b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f11b1cd-92f7-430d-ad60-c5d6b5483643",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df2 = add_ecdf_normalization(final_df1)\n",
    "final_df2.to_csv(final_combined_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6e78143-9516-46ae-a699-31500afaaadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45231"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840aec92-cf3c-40e4-83bd-77251e79ecf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c4c01d7-378a-4f9f-a1e2-7af80df7ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df3 = filter_short_abstracts(final_df2)\n",
    "final_df3.to_csv(final_combined_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ef519ca-7b40-4a08-9919-605c01e6d64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41772"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a7e45-c1a6-457f-aaed-e45f156a4945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d461c3d-f9e3-4642-8251-fd7f2c8dd55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df4 = categorize_fcr(final_df3)\n",
    "final_df4.to_csv(final_combined_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dfbba94-e9b4-45ee-9509-19ef5942753d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41772"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b2d223-3f38-471d-aa2f-fb3485c154f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b63df-6546-4a3b-94fc-61b1b38edd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df5 = balance_dataset(final_df4)\n",
    "final_df5.to_csv(final_combined_csv_path, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88de7436-1e1f-4ea7-80ff-1533334947e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38634"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07efa3ba-f0eb-467e-aa24-2f1b984f633e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa8e84bf-62fa-4e8f-bb86-68b1178acd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df6 = preprocess_dataset(final_df5)\n",
    "final_df6.to_csv(final_combined_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb15ea5f-ff49-400e-8f3e-618ce4a7d738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38634"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18ed48-e88b-4bb3-ab38-f79043bee24a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55b235d-d428-453b-969c-f0b06a8f47e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df7 = remove_empty_titles_abstracts(final_df6)\n",
    "final_df7.to_csv(final_combined_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41f67328-73e3-4a31-8c99-c68313d9731e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38034"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b4e14f-cd36-4848-a10c-216b666610fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d83e333-870e-41b1-9940-a87aef8593d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df8 = balance_dataset(final_df7)\n",
    "final_df8.to_csv(final_combined_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9bc6e5bd-c750-409c-822d-3a1cbeba3881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37827"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28929614-a3b5-449c-b2c1-f665a99d9cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
