# Predicting Scholarly Impact with Retrieval-Augmented LLMs

## Description:
This research investigates how Retrieval-Augmented Generation (RAG) combined with Large Language Models (LLMs) can enhance scholarly impact prediction. This study uses the text-based analysis of papers and dense retrieval method for LLM to predict a paperâ€™s **Field Citation Ratio (FCR)**, a normalized citation metric that accounts for variations across disciplines.  

The study implements two experimental frameworks: (1) Zero-shot prompting using LLMs (Llama 3, Mistral, and Gemma) to establish a baseline, and (2) Retrieval-augmented prediction, where the model retrieves relevant academic papers from a FAISS-based dense vector store before generating impact predictions. Also, we include self-consistency with a rag, where the model makes multiple predictions per test paper and selects the median value. Papers are represented using SciBERT embeddings, and a FAISS-based retrieval index enhances contextual understanding.  


---

Create a project directory: (Step 1):

mkdir impact-prediction-rag

cd impact-prediction-rag

install related libraries (Step 2):

pip install -r requirements.txt

Code Workflow & How to Run (Step 3):
ðŸ“‚ Dataset Preparation

    Download the file (research_papers.csv) from HuggingFace and save it in a folder called /dataset.
    The file has already been preprocessed and contains the necessary metadata for all research papers

ðŸ“‚ Running Prediction Models

    Zero-Shot Baseline Prediction:
    Run zero_shot.ipynb to predict FCR using LLM-only prompting (without retrieval).

    Using Retrieval-Augmented Prediction (RAG) with LLMs:

    Run each notebook: 
    Gemma: gemma.ipynb
    Llama 3: llama3.ipynb
    Mistral: mistral.ipynb


ðŸ“‚ Evaluating Model Performance:

    MAE, RMSE, and NDCG scores are automatically calculated after the model makes all predictions.
    Individual predictions for each model are stored in "results/predictions"
    Metrics for each approach (Zero-shot vs RAG) is stored in "results/metrics"


## Code and Datasets

```bash

impact-prediction-rag/
â”‚
â”œâ”€â”€ dataset/                            # Datasets for training & evaluation
â”‚   â”œâ”€â”€ research_papers.csv             # dataset containing research papers metadata
â”‚
â”œâ”€â”€ src/                                # Core implementation scripts
â”‚   â”œâ”€â”€ zero_shot
â”‚         â”œâ”€â”€ zero_shot.py              # Zero-shot LLM prediction module 
â”‚         â”œâ”€â”€ zero_shot.ipynb           # Zero-shot prompting notebook
â”‚   â”œâ”€â”€ gemma
â”‚        â”œâ”€â”€ gemma.py                   # Gemma-7b rag module
â”‚        â”œâ”€â”€ gemma.ipynb                # Gemma-7b model notebook
â”‚   â”œâ”€â”€ llama3
â”‚        â”œâ”€â”€ llama3.py                  # Llama 3-8b rag module
â”‚        â”œâ”€â”€ llama3.ipynb               # Llama 3-8b model notebook
â”‚   â”œâ”€â”€ mistral
â”‚        â”œâ”€â”€ mistral.py                 # Mistral rag module
â”‚        â”œâ”€â”€ mistral.ipynb              # Mistral model notebook
â”‚
â”‚
â”œâ”€â”€ results/                            # Model performance results
â”‚   â”œâ”€â”€ predictions
â”‚        â”œâ”€â”€ gemma rag predictions.csv                 
â”‚        â”œâ”€â”€ gemma zero-shot predictions.csv
â”‚        â”œâ”€â”€ llama3 rag predictions.csv                 
â”‚        â”œâ”€â”€ llama3 zero-shot predictions.csv
â”‚        â”œâ”€â”€ mistral rag predictions.csv                 
â”‚        â”œâ”€â”€ mistral zero-shot predictions.csv
â”‚   â”œâ”€â”€ metrics
â”‚        â”œâ”€â”€ zero-shot metrics.csv                 
â”‚        â”œâ”€â”€ gemma rag metrics.csv
â”‚        â”œâ”€â”€ llama3 rag metrics.csv
â”‚        â”œâ”€â”€ mistral rag metrics.csv
â”‚ 
â”‚
â”œâ”€â”€ requirements.txt           # List of dependencies
â”œâ”€â”€ README.md                  # Project documentation

'''
